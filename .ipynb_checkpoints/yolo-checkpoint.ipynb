{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5e0p0K0M9mVk"
   },
   "source": [
    "# Hướng dẫn cài đặt YOLO v1\n",
    "## Giới thiệu\n",
    "Trong notebook này, mình sẽ hướng dẫn các bạn cài đặt mô hình YOLO v1 cho tập dữ liệu mẫu được phát sinh. Sau khi làm xong, các bạn sẽ nắm được mô hình CNN, và ý nghĩa hàm loss, cách cài đặt, cũng như quá trình huấn luyện bằng tensorflow\n",
    "\n",
    "You only look once (YOLO) là một mô hình CNN để detect object mà ưu điểm nổi trội là nhanh hơn nhiều so với những mô hình cũ. Thậm chí có thể chạy tốt trên những thiết bị IOT có cấu hình yếu như raspberry pi.\n",
    "\n",
    "Đầu vào mô hình là một bức ảnh, đối với bài toán object detection, chúng ta không chỉ phải phân loại được object trên bức ảnh mà còn phải định vị được vị trí của đối tượng đó. Object Detection có khá nhiều ứng dụng, ví dụ như hệ thống theo dõi người dẫn của Trung Quốc, từ đó có thể giúp chính quyền xác định được tội phạm lẫn trốn ở đó hay không, hoặc hệ thống xe tự lái, cũng phải xác định được người đi đường ở đâu từ đó đưa ra quyết định di chuyển tiếp theo\n",
    "\n",
    "![yolo](https://pbcquoc.github.io/images/yolo_example.png)\n",
    "\n",
    "Có một số hướng tiếp cận để giải quyết vấn đề, đồng thời mỗi lần chạy tốn rất nhiều thời gian. Mình sẽ liệt kê ra để các bạn có thể nắm được ý tưởng để giải quyết bài toàn object detection.\n",
    "\n",
    "* Chia ảnh thành nhiều box, mỗi box các bạn sẽ detect object trong box đó. Vị trí của object chính là tạo độ của box đó.\n",
    "* Thay vì chia thành từng box, chúng ta sẽ sử dụng một thuật toán để lựa chọn những region ứng viên (ví dụ như là thuật toán Selective Search), các vùng ứng viên này các bạn có thể tưởng như là những vùng liên thông với nhau trên kênh màu RGB, sau đó với mỗi vùng ứng viên này, chúng ta dùng model để phân loại object. Chúng ta có một số mô hình xây dựng theo kiểu này như RCNN, Fast RCNN và Faster RCNN.\n",
    "\n",
    "Rất rõ ràng, nhược điểm của các phương pháp trên là tốn rất nhiều tài nguyên để tính toán cho mọi vùng trên một bức ảnh,và do đó không thể chạy realtime trên các thiết bị yếu.\n",
    "\n",
    "## Import các thư viện\n",
    "Thay vì khai báo các biến, định nghĩa các layer bằng tensorflow một cách chi tiết, chúng ta sử dụng high level api của trong gói slim để định gọi CNN một cách nhanh gọn hơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uUDueT3m9mVn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pbcquoc/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "## slim là package đi kèm với tensorflow, giúp định nghĩa nhanh các loại mô hình deep learning\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow.contrib.slim.nets\n",
    "from tensorflow.contrib.slim.nets import vgg \n",
    "## sklearn là một thư viện rất phổ biến trong ML, chúng ta chỉ sử dụng tran_test_split để chia data thành 2 tập\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "## thư viện tính toán trên matrix\n",
    "import numpy as np\n",
    "import cv2\n",
    "# thư viện hiển thị biểu đồ\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5mUAOfH49mVv"
   },
   "source": [
    "### Grid System\n",
    "Một trong những khái niệm quan trọng nhất của YOLO là grid System. Cụ thể, ảnh được chia thành ma trận ô vuông 7x7, mỗi ô vuông bao gồm một tập các thông tin mà mô hình phải dữ đoán.\n",
    "![grid system](https://pbcquoc.github.io/images/yolo_grid_system.png)\n",
    "\n",
    "* Đối tượng duy nhất mà ô vuông đó chứa. Tâm của đối tượng cần xác định nằm trong ô vuông nào thì ô vuông đó chứa đối tượng đó. Ví dụ tâm của cô gái nằm trong ô vuông màu xanh, do đó mô hình phải dự đoán được nhãn của ô vuông đó là cô gái. Lưu ý, cho dù phần ảnh cô gái có nằm ở ô vuông khác mà tâm không thuộc ô vuông đó thì vẫn không tính là chứa cô gái, ngoài ra, nếu có nhiều tâm nằm trong một ô vuông thì chúng ta vẫn chỉ gán một nhãn cho ô vuông đó thôi. Chính ràng buột mỗi ô vuông chỉ chứa một đối tượng là nhược điểm của mô hình này. Nó làm cho ta không thể detect những object có tầm nằm cùng một ô vuông. Tuy nhiên chúng ta có thể tăng grid size từ 7x7 lên kích thước lớn hơn để có thể detect được nhiều object hơn. Ngoài ra, kích thước của ảnh đầu vào phải là bội số của grid size.\n",
    "* Mỗi ô vuông chịu trách nhiệm dự đoán 2 boundary box của đối tượng. Mỗi boundary box dữ đoán có chứa object hay không và thông tin vị trí của boundary box gồm trung tâm boundary box của đối tượng và chiều dài, rộng của boundary box đó. Ví vụ ô vuông màu xanh cần dự đoán 2 boundary box chứa cô gái như hình minh họa ở dưới. Một điều cần lưu ý, lúc cài đặt chúng ta không dự đoán giá trị pixel mà cần phải chuẩn hóa kích thước ảnh về đoạn từ [0-1] và dự đoán độ lệch của tâm đối tượng đến box chứa đối tượng đó. Ví dụ, chúng ta thay vì dữ đoán vị trí pixel của điểm màu đỏ, thì cần dự đoán độ lệch a,b trong ô vuông chứa tâm object.\n",
    "\n",
    "![2box](https://pbcquoc.github.io/images/yolo_2box.png)\n",
    "\n",
    "Tổng hợp lại, với mỗi ô vuông chúng ta cần dữ đoán các thông tin sau :\n",
    "\n",
    "* Ô vuông có chứa đối tượng nào hay không?\n",
    "* Dự đoán độ lệch 2 box chứa object so với ô vuông hiện tại\n",
    "* Lớp của object đó\n",
    "Như vậy với mỗi ô vuông chúng ta cần dữ đoán một vector có (nbox+4*nbox+nclass) chiều. Ví dụ, chúng ta cần dự đoán 2 box, và 3 lớp đối với mỗi ô vuông thì chúng sẽ có một ma trận 3 chiều 7x7x13 chứa toàn bộ thông tin cần thiết.\n",
    "\n",
    "![label](https://pbcquoc.github.io/images/yolo_predict_vector.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MIKQbEdU9mVw"
   },
   "outputs": [],
   "source": [
    "# kích thước grid system \n",
    "cell_size = 7 \n",
    "# số boundary box cần dự đoán mỗi ô vuông\n",
    "box_per_cell = 2\n",
    "# kích thước ảnh đầu vào\n",
    "img_size = 224\n",
    "# số loại nhãn\n",
    "classes = {'circle':0, 'triangle':1,  'rectangle':2}\n",
    "nclass = len(classes)\n",
    "\n",
    "box_scale = 5.0\n",
    "noobject_scale = 0.5\n",
    "batch_size = 128\n",
    "# số lần huấn luyện\n",
    "epochs = 10\n",
    "# learning của chúng ta\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ERasMn1G9mVy"
   },
   "source": [
    "## Load tập dữ liệu\n",
    "Chúng ta có 25k mẫu dữ liệu, mỗi ảnh có kích thước 224x224, và một file json chứa nhãn của các ảnh, gồm tập các object tương ứng với vị trí tạo độ của boundary box chứa object.\n",
    "\n",
    "![dataset](https://raw.githubusercontent.com/pbcquoc/yolo/master/image/dataset.png)\n",
    "\n",
    "Để chuẩn bị dữ liệu chúng ta cần load tất cả các ảnh vào bộ nhớ, chuyển thông tin tạo độ của boundary box thành thông tin nhãn tương ứng với quy định ở trên. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mPSdYB0F9mVz"
   },
   "outputs": [],
   "source": [
    "def load():\n",
    "    labels = json.load(open('train/labels.json'))\n",
    "    # số lương ảnh\n",
    "    N = len(labels)\n",
    "    # matrix chứa ảnh\n",
    "    X = np.zeros((N, img_size, img_size, 3), dtype='uint8')\n",
    "    # matrix chứa nhãn của ảnh tương ứng\n",
    "    y = np.zeros((N,cell_size, cell_size, 5+nclass))\n",
    "    for idx, label in enumerate(labels):\n",
    "        img = cv2.imread(\"train/{}.png\".format(idx))\n",
    "        # normalize về khoảng [0-1]\n",
    "        X[idx] = img\n",
    "        for box in label['boxes']:\n",
    "            x1, y1 = box['x1'], box['y1']\n",
    "            x2, y2 = box['x2'], box['y2']\n",
    "            # one-hot vector của nhãn object\n",
    "            cl = [0]*len(classes)\n",
    "            cl[classes[box['class']]] = 1\n",
    "            # tâm của boundary box\n",
    "            x_center, y_center, w, h = (x1+x2)/2.0, (y1+y2)/2.0, x2-x1, y2-y1\n",
    "            # index của object trên ma trận ô vuông 7x7\n",
    "            x_idx, y_idx = int(x_center/img_size*cell_size), int(y_center/img_size*cell_size)\n",
    "            # gán nhãn vào matrix \n",
    "            y[idx, y_idx, x_idx] = 1, x_center, y_center, w, h, *cl\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rj9KKeC99mV2"
   },
   "source": [
    "### Download tập dữ liệu trên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32791,
     "status": "ok",
     "timestamp": 1545695464701,
     "user": {
      "displayName": "Quoc Pham",
      "photoUrl": "https://lh3.googleusercontent.com/-fhmgGol9Tbo/AAAAAAAAAAI/AAAAAAAAAoE/dyUSaTspIFQ/s64/photo.jpg",
      "userId": "10232171753645408525"
     },
     "user_tz": -420
    },
    "id": "MxvjOzQ79mV3",
    "outputId": "265d5936-5f94-4713-e123-fb2d1694b32a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.png\r\n",
      "10000.png\r\n",
      "10001.png\r\n",
      "10002.png\r\n",
      "10003.png\r\n",
      "10004.png\r\n",
      "10005.png\r\n",
      "10006.png\r\n",
      "10007.png\r\n",
      "10008.png\r\n",
      "ls: write error: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "! wget --quiet --no-check-certificate 'https://docs.google.com/uc?export=download&id=12sZLOe5VDvAqGHcjJh7mVmjB6HPeIEJh' -O train.zip\n",
    "! unzip -o -q train.zip\n",
    "! ls train | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLh8r9VM9mV6"
   },
   "source": [
    "## Chia tập dữ liệu thành train/val\n",
    "train_test_split giúp chia dữ liệu thành 2 tập train và test trong một nốt nhạc. Tập train dùng để huấn luyện mô hình, tập test dùng để đánh giá mô hình bằng các metric ví dụ như accuracy, iou, đồng thời quan sát quá trình overfit. random_state giúp reproduce kết quả của những luấn huấn luyện tiếp theo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UgwY4lon9mV7"
   },
   "outputs": [],
   "source": [
    "X, y = load()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m3F0FHyy9mV9"
   },
   "source": [
    "## Định nghĩa mô hình CNN\n",
    "Chúng ta đã cần biết phải dự đoán những thông tin nào đối với mỗi ô vuông, điều quan trọng tiếp theo là xây dựng một mô hình CNN có cho ra ouput với shape phù hợp theo yêu cầu của chúng ta, tức là gridsize x gridsize x (nbox+4*nbox+nclass). Ví dụ với gridsize là 7x7 là mỗi ô vuông dự đoán 2 boxes, và có 3 loại object tất cả thì chúng ta phải cần output có shape 7x7x13 từ mô hình CNN\n",
    "\n",
    "![cnn](https://pbcquoc.github.io/images/yolo_cnn.jpeg)\n",
    "\n",
    "YOLO sử dụng linear regression để dự đoán các thông tin ở mỗi ô vuông. Do đó, ở layer cuối cùng chúng ta sẽ không sử dụng bất kì hàm kích hoạt nào cả. Với ảnh đầu vào là 448x448, mô hình CNN có 6 tầng max pooling với size 2x2 sẽ giảm 64 lần kích thước ảnh xuống còn 7x7 ở output đầu ra. Đồng thời thay vì sử dụng tầng full connected ở các tầng cuối cùng, chúng ta có thể thay thế bằng tầng 1x1 conv với 13 feature maps để output shape dễ dàng cho ra 7x7x13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jxklBSPf9mV-"
   },
   "outputs": [],
   "source": [
    "def vgg16(inputs, is_training):\n",
    "    \"\"\"định nghĩa CNN\n",
    "    Args:\n",
    "      inputs: 5-D tensor [batch_size, width, height, 3]\n",
    "    Return:\n",
    "      iou: 4-D tensor [batch_size, 7, 7, 5*nbox + nclass]\n",
    "    \"\"\"\n",
    "    # khái báo scope để có thê group những biến liên quan cho việc visualize trên tensorboard.\n",
    "    with tf.variable_scope(\"vgg_16\"):\n",
    "        with slim.arg_scope(vgg.vgg_arg_scope()):\n",
    "            # hàm repeat có tác dụng lặp lại tầng conv2d n lần mà không phải định nghĩa phức tạp. thank for slim package\n",
    "            net = slim.repeat(inputs, 2, slim.conv2d, 16, [3, 3], scope='conv1')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 32, [3, 3], scope='conv2')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 64, [3, 3], scope='conv3')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv4')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 256, [3, 3], scope='conv5')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "            \n",
    "            # thay vì sử dụng 2 tầng fully connected tại đây, \n",
    "            # chúng ta sử dụng conv với kernel_size = (1,1) có tác dụng giống hệt tầng fully conntected\n",
    "            net = slim.conv2d(net, 512, [1, 1], scope='fc6')   \n",
    "\n",
    "            net = slim.conv2d(net, 13, [1, 1], activation_fn=None, scope='fc7')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4btfv-ip9mWB"
   },
   "source": [
    "## Hàm tính IOU \n",
    "là hàm rất quan trọng của mô hình YOLO, giúp chúng ta đánh giá các boundary box được dự đoán có chính xác hay không\n",
    "\n",
    "![iou](https://pbcquoc.github.io/images/yolo_iou.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mKqBZTTc9mWC"
   },
   "outputs": [],
   "source": [
    "def compute_iou(boxes1, boxes2, scope='iou'):\n",
    "    \"\"\"calculate ious\n",
    "    Args:\n",
    "      boxes1: 5-D tensor [BATCH_SIZE, CELL_SIZE, CELL_SIZE, BOXES_PER_CELL, 4]  ====> (x_center, y_center, w, h)\n",
    "      boxes2: 5-D tensor [BATCH_SIZE, CELL_SIZE, CELL_SIZE, BOXES_PER_CELL, 4] ===> (x_center, y_center, w, h)\n",
    "    Return:\n",
    "      iou: 4-D tensor [BATCH_SIZE, CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        # transform (x_center, y_center, w, h) to (x1, y1, x2, y2)\n",
    "        boxes1_t = tf.stack([boxes1[..., 0] - boxes1[..., 2] / 2.0,\n",
    "                             boxes1[..., 1] - boxes1[..., 3] / 2.0,\n",
    "                             boxes1[..., 0] + boxes1[..., 2] / 2.0,\n",
    "                             boxes1[..., 1] + boxes1[..., 3] / 2.0],\n",
    "                            axis=-1)\n",
    "\n",
    "        boxes2_t = tf.stack([boxes2[..., 0] - boxes2[..., 2] / 2.0,\n",
    "                             boxes2[..., 1] - boxes2[..., 3] / 2.0,\n",
    "                             boxes2[..., 0] + boxes2[..., 2] / 2.0,\n",
    "                             boxes2[..., 1] + boxes2[..., 3] / 2.0],\n",
    "                            axis=-1)\n",
    "\n",
    "        # calculate the left up point & right down point\n",
    "        lu = tf.maximum(boxes1_t[..., :2], boxes2_t[..., :2])\n",
    "        rd = tf.minimum(boxes1_t[..., 2:], boxes2_t[..., 2:])\n",
    "\n",
    "        # intersection\n",
    "        intersection = tf.maximum(0.0, rd - lu)\n",
    "        inter_square = intersection[..., 0] * intersection[..., 1]\n",
    "\n",
    "        # calculate the boxs1 square and boxs2 square\n",
    "        square1 = boxes1[..., 2] * boxes1[..., 3]\n",
    "        square2 = boxes2[..., 2] * boxes2[..., 3]\n",
    "\n",
    "        union_square = tf.maximum(square1 + square2 - inter_square, 1e-10)\n",
    "\n",
    "    return tf.clip_by_value(inter_square / union_square, 0.0, 1.0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VWkGYaji9mWE"
   },
   "source": [
    "## Loss function\n",
    "\n",
    "Chúng ta đã định nghĩa được những thông tin mà mô hình cần phải dự đoán, và kiến trúc của mô hình CNN. Bây giờ là lúc mà chúng ta sẽ định nghĩa hàm lỗi.\n",
    "\n",
    "YOLO sử dụng hàm độ lỗi bình phương giữ dự đoán và nhãn để tính độ lỗi cho mô hình. Cụ thể, độ lỗi tổng của chúng ta sẽ là tổng của 3 độ lỗi con sau:\n",
    "* Độ lỗi của việc dữ đoán loại nhãn của object - Classifycation loss\n",
    "* Độ lỗi của dự đoán tạo độ cũng như chiều dài, rộng của boundary box - Localization loss\n",
    "* Độ lỗi của ô vuông có chứa object nào hay không - Confidence loss\n",
    "\n",
    "Chúng ta mong muốn hàm lỗi có chức năng sau. Trong quá trình huấn luyện, mô hình sẽ nhìn vào những ô vuông có chứa object. Tăng classification score lớp đúng của object đó lên. Sau đó, cũng nhìn vào ô vuông đó, tìm boundary box tốt nhất trong 2 boxes được dự đoán. Tăng localization score của boundary box đó lên, thay đổi thông tin boundary box để gần đúng với nhãn. Đối với những ô vuông không chứa object, giảm confidence score và chúng ta sẽ không quan tâm đến classification score và localization score của những ô vuông này.\n",
    "\n",
    "Tiếp theo, chúng ta sẽ đi lần lượt vào chi tiết ý nghĩa của các độ lỗi trên.\n",
    "\n",
    "### Classification Loss\n",
    "Chúng ta chỉ tính classification loss cho những ô vuông được đánh nhãn là có object. Classification loss tại những ô vuông đó được tính bằng đỗ lỗi bình phương giữa nhãn được dự đoán và nhãn đúng của nó.\n",
    "\n",
    "![classification_loss](https://github.com/pbcquoc/yolo/raw/master/image/classification_loss.png)\n",
    "\n",
    "![loss](https://pbcquoc.github.io/images/yolo_classification_loss.png)\n",
    "\n",
    "Ví dụ, trong hình minh họa ở trên, chúng ta có 2 object tại ô vuông (dòng,cột) là (2,1) và (3,4), chứa object là hình tam giác và hình tức giác đều. Độ lỗi classification loss chỉ tính cho 2 object này mà ko quan tâm đến những ô vuông khác. Lúc cài đặt chúng ta cần lưu ý phải nhân với một mask để triệt tiêu giá trị lỗi tại những ô vuông ko quan tâm.\n",
    "\n",
    "### Localization Loss\n",
    "Localization loss dùng để tính giá trị lỗi cho boundary box được dự đoán bao gồm offset x,y và chiều dài, rộng so với nhãn chính xác của chúng ta. Các bạn nên lưu ý rằng, chúng ta không tính toán trực tiếp giá trị lỗi này trên kích thước của ảnh mà cần chuẩn dưới kính thước ảnh về đoạn [0-1] đối với tọa độ điểm tâm, và không dữ đoán trực tiếp điểm tâm mà phải dự đoán giá trị lệch offset x,y so với ô vuông tương ứng. Việc chuẩn hóa kích thước ảnh và dự đoán offset làm cho mô hình nhanh hội tụ hơn so với việc dự đoán giá trị mặc định.\n",
    "![localization_loss](https://github.com/pbcquoc/yolo/raw/master/image/localization_loss.png)\n",
    "\n",
    "\n",
    "Độ lỗi localization loss được tính bằng tổng đỗ lỗi bình phương của offsetx, offsety và chiều dài, rộng trên tất cả các ô vuông có chứa object. Tại mỗi ô vuông đúng,ta chọn 1 boundary box có IOU (Intersect over union) tốt nhất, rồi sau đó tính độ lỗi theo các boundary box này. Theo hình mình họa trên chúng ta có 4 boundary box tại ô vuông đúng có viền màu đỏ, chúng ta chọn 1 box tại mỗi ô vuông để tính độ lỗi. Còn box xanh được bỏ qua.\n",
    "\n",
    "Localization loss là độ lỗi quan trọng nhất trong 3 loại độ lỗi trên. Do đó, ta cần đặt trọng số cao hơn cho độ lỗi này.\n",
    "\n",
    "### Confidence Loss\n",
    "Confidence loss thể hiện độ lỗi giữa dự đoán boundary box đó chứa object so với nhãn thực tế tại ô vuông đó. Độ lỗi này tính nên cả những ô vuông chứa object và không chứa object.\n",
    "\n",
    "![confidence_loss](https://github.com/pbcquoc/yolo/raw/master/image/confidence_loss.png)\n",
    "\n",
    "Độ lỗi này là độ lỗi bình phường của dự đoán boundary đó chứa object với nhãn thực tế của ô vuông tại vị trí tương ứng, chúng ta lưu ý rằng, độ lỗi tại ô vuông mà nhãn chứa object quan trọng hơn là độ lỗi tại ô vuông không chứa object, do đó chúng ta cần sử dụng hệ số lambda để cân bằng điều này.\n",
    "\n",
    "### Total loss\n",
    "Tổng kết lại, tổng lỗi của chúng ta sẽ bằng tổng của 3 loại độ lỗi trên\n",
    "![total_loss](https://github.com/pbcquoc/yolo/raw/master/image/total_loss.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-HEcGXqq9mWF"
   },
   "outputs": [],
   "source": [
    "def loss_layer(predicts, labels, scope='loss_layer'):\n",
    "    \"\"\"calculate loss function\n",
    "    Args:\n",
    "      predicts: 4-D tensor [batch_size, 7, 7, 5*nbox+n_class] \n",
    "      labels: 4-D tensor [batch_size, 7, 7, 5+n_class]\n",
    "    Return:\n",
    "      loss: scalar\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        offset = np.transpose(np.reshape(np.array(\n",
    "            [np.arange(cell_size)] * cell_size * box_per_cell),\n",
    "            (box_per_cell, cell_size, cell_size)), (1, 2, 0))\n",
    "        offset = offset[None, :]\n",
    "        offset = tf.constant(offset, dtype=tf.float32)\n",
    "        offset_tran = tf.transpose(offset, (0, 2, 1, 3))\n",
    "        \n",
    "        # 2 phần tử đầu của vector dự đoán tại một ô vuông là confidence score\n",
    "        predict_object = predicts[..., :box_per_cell]\n",
    "        \n",
    "        # 8 phần tử tiếp theo là dự đoán offset của boundary box và width height\n",
    "        predict_box_offset = tf.reshape(predicts[...,box_per_cell:5*box_per_cell], (-1, cell_size, cell_size, box_per_cell, 4))\n",
    "        \n",
    "        # các phần tử cuối là dự đoán lớp của object\n",
    "        predict_class = predicts[...,5*box_per_cell:]\n",
    "        \n",
    "        # chuyển vị trí offset về toạ độ normalize trên khoảng [0-1]\n",
    "        predict_normalized_box = tf.stack(\n",
    "                                    [(predict_box_offset[..., 0] + offset) / cell_size,\n",
    "                                     (predict_box_offset[..., 1] + offset_tran) / cell_size,\n",
    "                                     tf.square(predict_box_offset[..., 2]),\n",
    "                                    tf.square(predict_box_offset[..., 3])], axis=-1)\n",
    "\n",
    "        # lấy các nhãn tương ứng \n",
    "        true_object = labels[..., :1]\n",
    "        true_box = tf.reshape(labels[..., 1:5], (-1, cell_size, cell_size, 1, 4))\n",
    "        \n",
    "        # để normalize tọa độ pixel về đoạn [0-1] chúng ta chia cho img_size (224)\n",
    "        true_normalized_box = tf.tile(true_box, (1, 1, 1, box_per_cell, 1))/img_size\n",
    "        true_class = labels[..., 5:]\n",
    "        \n",
    "        # tính vị trí offset từ nhãn \n",
    "        true_box_offset =  tf.stack(\n",
    "                                    [true_normalized_box[..., 0] * cell_size - offset,\n",
    "                                     true_normalized_box[..., 1] * cell_size - offset_tran,\n",
    "                                     tf.sqrt(true_normalized_box[..., 2]),\n",
    "                                     tf.sqrt(true_normalized_box[..., 3])], axis=-1)\n",
    "        \n",
    "        # tính iou\n",
    "        predict_iou = compute_iou(true_normalized_box, predict_normalized_box)\n",
    "        \n",
    "        # mask chứa vị trí các ô vuông chứa object\n",
    "        object_mask = tf.reduce_max(predict_iou, 3, keepdims=True)  \n",
    "        \n",
    "        # tính metric để monitor \n",
    "        iou_metric = tf.reduce_mean(tf.reduce_sum(object_mask, axis=[1,2,3])/tf.reduce_sum(true_object, axis=[1,2,3]))\n",
    "        \n",
    "        object_mask = tf.cast((predict_iou>=object_mask), tf.float32)*true_object\n",
    "\n",
    "        noobject_mask = tf.ones_like(object_mask) - object_mask\n",
    "        \n",
    "        ## class loss\n",
    "        class_delta = true_object*(predict_class - true_class)\n",
    "        class_loss = tf.reduce_mean(tf.reduce_sum(tf.square(class_delta), axis=[1,2,3]), name='class_loss')\n",
    "        \n",
    "        ## object loss\n",
    "        object_delta = object_mask*(predict_object - predict_iou)\n",
    "        object_loss = tf.reduce_mean(tf.reduce_sum(tf.square(object_delta), axis=[1,2,3]), name='object_loss')\n",
    "        \n",
    "        ## noobject loss\n",
    "        noobject_delta = noobject_mask*predict_object\n",
    "        noobject_loss = tf.reduce_mean(tf.reduce_sum(tf.square(noobject_delta), axis=[1,2,3]), name='noobject_loss')\n",
    "        \n",
    "        ## coord loss\n",
    "        box_mask = tf.expand_dims(object_mask, 4)\n",
    "        box_delta = box_mask*(predict_box_offset - true_box_offset)\n",
    "        box_loss = tf.reduce_mean(tf.reduce_sum(tf.square(box_delta), axis=[1,2,3]), name='box_loss')\n",
    "        \n",
    "        loss = 0.5*class_loss + object_loss + 0.1*noobject_loss + 10*box_loss\n",
    "        \n",
    "        return loss, iou_metric, predict_object, predict_class, predict_normalized_box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8XHOBghk9mWH"
   },
   "source": [
    "## Biên dịch graph\n",
    "Phần khó đã qua, chúng mừng các bạn!!\n",
    "\n",
    "Bây giờ là biên dịch graph và định nghĩa optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R8MEe8sH9mWI"
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():    \n",
    "    # None đại diện cho batch_size, giúp batch_size có thể thay đổi linh hoạt\n",
    "    images = tf.placeholder(\"float\", [None, img_size, img_size, 3], name=\"input\")\n",
    "    labels = tf.placeholder('float', [None, cell_size, cell_size, 8], name='label')\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "    logits = vgg16(images, is_training)\n",
    "    loss, iou_metric, predict_object, predict_class, predict_normalized_box = loss_layer(logits, labels)\n",
    "    \n",
    "    # định nghĩa adam optimizer, để tối ưu hàm loss\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kx5eR9j09mWK"
   },
   "source": [
    "## Training\n",
    "Phần này chúng ta sẽ bắt đầu training, và tính iou metric để quan sát, iou nhận giá trị từ [0-1], sấp xỉ 1 là rất tốt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ihiosiV-9mWL",
    "outputId": "9c7f2b92-db44-48ce-896e-b6818bd60b67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - running_time: 100s - train_loss: 0.608 - train_iou: 0.536 - val_loss: 0.651 - val_iou: 0.521\n",
      "epoch: 1 - running_time: 100s - train_loss: 0.269 - train_iou: 0.656 - val_loss: 0.287 - val_iou: 0.649\n",
      "epoch: 2 - running_time: 101s - train_loss: 0.205 - train_iou: 0.656 - val_loss: 0.212 - val_iou: 0.688\n",
      "epoch: 3 - running_time: 103s - train_loss: 0.155 - train_iou: 0.715 - val_loss: 0.174 - val_iou: 0.737\n",
      "epoch: 4 - running_time: 101s - train_loss: 0.140 - train_iou: 0.710 - val_loss: 0.164 - val_iou: 0.721\n",
      "epoch: 5 - running_time: 100s - train_loss: 0.120 - train_iou: 0.764 - val_loss: 0.139 - val_iou: 0.712\n",
      "epoch: 6 - running_time: 100s - train_loss: 0.109 - train_iou: 0.785 - val_loss: 0.128 - val_iou: 0.723\n",
      "epoch: 7 - running_time: 100s - train_loss: 0.107 - train_iou: 0.802 - val_loss: 0.131 - val_iou: 0.766\n",
      "epoch: 8 - running_time: 100s - train_loss: 0.099 - train_iou: 0.777 - val_loss: 0.116 - val_iou: 0.766\n",
      "epoch: 9 - running_time: 103s - train_loss: 0.086 - train_iou: 0.793 - val_loss: 0.110 - val_iou: 0.783\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # định nghĩa saver để lưu lại trọng số của mô hình, dùng trong test các ảnh mới\n",
    "    saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        for batch in range(len(X_train)//batch_size):\n",
    "            # lấy từng batch, forward, backward, cập nhật trọng số theo adam optimizer\n",
    "            X_batch = X_train[batch*batch_size:(batch+1)*batch_size]\n",
    "            y_batch = y_train[batch*batch_size:(batch+1)*batch_size]\n",
    "            train_total_loss, train_iou_m,_ = sess.run([loss, iou_metric, train_op], {images:X_batch, labels:y_batch, is_training:True})            \n",
    "        end_time = time.time()\n",
    "        \n",
    "        # tính toán loss, iou trên tập validation\n",
    "        val_loss = []\n",
    "        val_iou_ms = []\n",
    "        for batch in range(len(X_test)//batch_size):\n",
    "            val_X_batch = X_test[batch*batch_size:(batch+1)*batch_size]\n",
    "            val_y_batch = y_test[batch*batch_size:(batch+1)*batch_size]\n",
    "            total_val_loss, val_iou_m, val_predict_object, val_predict_class, val_predict_normalized_box = sess.run([loss, iou_metric, predict_object, predict_class, predict_normalized_box], \n",
    "                                                 {images:val_X_batch, labels:val_y_batch, is_training:False})\n",
    "            val_loss.append(total_val_loss)\n",
    "            val_iou_ms.append(val_iou_m)\n",
    "            \n",
    "        saver.save(sess, './model/yolo', global_step=epoch)\n",
    "        print('epoch: {} - running_time: {:.0f}s - train_loss: {:.3f} - train_iou: {:.3f} - val_loss: {:.3f} - val_iou: {:.3f}'.format(epoch, end_time - start_time, train_total_loss, train_iou_m, np.mean(val_loss), np.mean(val_iou_ms)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KtW4sLFh9mWO"
   },
   "source": [
    "## Hiển thị kết quả dự đoán\n",
    "Filter tất cả các box không thỏa điều kiện như ko chứa object, merge các box overlap nhiều"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DnEphdfj9mWO"
   },
   "outputs": [],
   "source": [
    "def iou(box1, box2):\n",
    "    \"\"\" tính iou bằng numpy \n",
    "    Args:\n",
    "      box1: [center_x, center_y, w, h] \n",
    "      box2: [center_x, center_y, w, h] \n",
    "    Return:\n",
    "      iou: iou\n",
    "    \"\"\"\n",
    "    tb = min(box1[0] + 0.5 * box1[2], box2[0] + 0.5 * box2[2]) - \\\n",
    "        max(box1[0] - 0.5 * box1[2], box2[0] - 0.5 * box2[2])\n",
    "    lr = min(box1[1] + 0.5 * box1[3], box2[1] + 0.5 * box2[3]) - \\\n",
    "        max(box1[1] - 0.5 * box1[3], box2[1] - 0.5 * box2[3])\n",
    "    inter = 0 if tb < 0 or lr < 0 else tb * lr\n",
    "    return inter / (box1[2] * box1[3] + box2[2] * box2[3] - inter)\n",
    "    \n",
    "def interpret_output(predict_object, predict_class, predict_normalized_box):\n",
    "    # nhận lại img-size để ra không gian pixel\n",
    "    predict_box= predict_normalized_box*img_size\n",
    "    predict_object = np.expand_dims(predict_object, axis=-1)\n",
    "    predict_class = np.expand_dims(predict_class, axis=-2)\n",
    "    # xác suất ô boundary chứa class bằng boundary chứa object * xác suất có điều kiện của lớp đó mà ô vuông chứa object\n",
    "    class_probs = predict_object*predict_class\n",
    "    \n",
    "    # giữ các boundary box mà có xác suất chứa lớp >= 0.2\n",
    "    filter_mat_probs = np.array(class_probs >= 0.2, dtype='bool')\n",
    "    filter_mat_boxes = np.nonzero(filter_mat_probs)\n",
    "    boxes_filtered = predict_box[filter_mat_boxes[0], filter_mat_boxes[1], filter_mat_boxes[2]]\n",
    "    class_probs_filtered = class_probs[filter_mat_probs]\n",
    "    \n",
    "    # chọn index của lớp có xác xuất lớp nhất lại mỗi boundary box\n",
    "    classes_num_filtered = np.argmax(\n",
    "        filter_mat_probs, axis=3)[\n",
    "        filter_mat_boxes[0], filter_mat_boxes[1], filter_mat_boxes[2]]\n",
    "\n",
    "    # giữ lại boundary box dự đoán có xác xuất lớp nhất\n",
    "    argsort = np.array(np.argsort(class_probs_filtered))[::-1]\n",
    "    boxes_filtered = boxes_filtered[argsort]\n",
    "    class_probs_filtered = class_probs_filtered[argsort]\n",
    "    classes_num_filtered = classes_num_filtered[argsort]\n",
    "\n",
    "    # thuật toán non-maximun suppression\n",
    "    for i in range(len(boxes_filtered)):\n",
    "        if class_probs_filtered[i] == 0:\n",
    "            continue\n",
    "        for j in range(i + 1, len(boxes_filtered)):\n",
    "            if iou(boxes_filtered[i], boxes_filtered[j]) > 0.5:\n",
    "                class_probs_filtered[j] = 0.0\n",
    "                \n",
    "    # filter bước cuối bỏ những boundary overlap theo thuật toán trên\n",
    "    filter_iou = np.array(class_probs_filtered > 0.0, dtype='bool')\n",
    "    boxes_filtered = boxes_filtered[filter_iou]\n",
    "    class_probs_filtered = class_probs_filtered[filter_iou]\n",
    "    classes_num_filtered = classes_num_filtered[filter_iou]\n",
    "\n",
    "    result = []\n",
    "    for i in range(len(boxes_filtered)):\n",
    "        result.append(\n",
    "            [classes_num_filtered[i],\n",
    "             boxes_filtered[i][0],\n",
    "             boxes_filtered[i][1],\n",
    "             boxes_filtered[i][2],\n",
    "             boxes_filtered[i][3],\n",
    "             class_probs_filtered[i]])\n",
    "\n",
    "    return result\n",
    "\n",
    "def draw_result(img, result):\n",
    "    \"\"\" hiển thị kết quả dự đoán\n",
    "    Args:\n",
    "      img: ảnh      \n",
    "      result: giá trị sinh ra ở hàm trên    \n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,10), dpi=40)\n",
    "    img = np.pad(img, [(50,50), (50,50), (0,0)], mode='constant', constant_values=255)\n",
    "    for i in range(len(result)):\n",
    "        x = int(result[i][1])+50\n",
    "        y = int(result[i][2])+50\n",
    "        w = int(result[i][3] / 2)\n",
    "        h = int(result[i][4] / 2)\n",
    "        cv2.rectangle(img, (x - w, y - h), (x + w, y + h), (231, 76, 60), 2)\n",
    "        cv2.rectangle(img, (x - w, y - h - 20),\n",
    "                      (x -w + 50, y - h), (46, 204, 113), -1)\n",
    "        cv2.putText(\n",
    "            img, '{} : {:.2f}'.format(result[i][0] ,result[i][5]),\n",
    "            (x - w + 5, y - h - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.3,\n",
    "            (0, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bpSEpEbI9mWQ"
   },
   "source": [
    "Chọn một từ tập validation, rồi hiển thị "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h2ynBhLo9mWR"
   },
   "outputs": [],
   "source": [
    "img_idx = 15\n",
    "result = interpret_output(val_predict_object[img_idx], val_predict_class[img_idx], val_predict_normalized_box[img_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YcRswuWV9mWT"
   },
   "source": [
    "Hiển thị kết quả dự đoán cùng boundary của dự đoán mới mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Y2NlwHC9mWU"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_result(val_X_batch[img_idx]*255, result)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rjC-BEK9mWa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "solution-yolo.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
